#!/usr/bin/env python3
"""
Careerjet Aggregator for GlobalJobHunter
"""

import os
import requests
import time
from datetime import datetime
from typing import List, Dict, Optional, Set
import hashlib
from dotenv import load_dotenv
import logging

# --- –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑ adzuna_aggregator ---
from adzuna_aggregator import JobVacancy, CacheManager, RateLimiter, GlobalJobAggregator

# --- –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã ---
from base_aggregator import BaseJobAggregator

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CareerjetAggregator(BaseJobAggregator):
    """
    –ê–≥—Ä–µ–≥–∞—Ç–æ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π —á–µ—Ä–µ–∑ Careerjet API.
    - –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–æ–∏—Å–∫ –∫–∞–∫ –ø–æ —Å—Ç—Ä–∞–Ω–µ, —Ç–∞–∫ –∏ –ø–æ –≥–æ—Ä–æ–¥—É.
    - –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∫–∞–∂–¥–æ–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –¥–ª—è –±–æ–ª—å—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏.
    """
    
    # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    MAX_PAGES_PER_SEARCH = 10  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
    JOBS_PER_PAGE = 50  # –û–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    REQUEST_TIMEOUT = 15  # –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    DELAY_BETWEEN_PAGES = 0.5  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
    
    def __init__(self, adzuna_countries: Dict, specific_jobs_map: Dict, cache_duration_hours: int = 12):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞.
        """
        super().__init__(source_name='Careerjet')
        # –ú–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å https –µ—Å–ª–∏ API –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç
        self.base_url = "http://public.api.careerjet.net/search"
        
        self.affid = os.getenv('CAREERJET_AFFID')
        if not self.affid:
            raise ValueError("CAREERJET_AFFID –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ!")

        self.cache_manager = CacheManager(cache_duration_hours=cache_duration_hours)
        self.rate_limiter = RateLimiter(requests_per_minute=25)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º frozenset –¥–ª—è –Ω–µ–∏–∑–º–µ–Ω—è–µ–º–æ—Å—Ç–∏ –∏ –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.country_map = {
            'gb': 'United Kingdom', 'us': 'United States', 'de': 'Germany',
            'fr': 'France', 'es': 'Spain', 'it': 'Italy', 'nl': 'Netherlands',
            'pl': 'Poland', 'ca': 'Canada', 'au': 'Australia', 'at': 'Austria',
            'ch': 'Switzerland', 'be': 'Belgium', 'se': 'Sweden', 'no': 'Norway',
            'dk': 'Denmark', 'cz': 'Czech Republic', 'sk': 'Slovakia'
        }

        self.adzuna_countries = adzuna_countries
        self.specific_jobs_map = specific_jobs_map
        
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–µ–º user-agent –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self.user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'
        
        # –°–æ–∑–¥–∞–µ–º Session –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": self.user_agent})

        logger.info(f"‚úÖ Careerjet Aggregator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (affid: ...{self.affid[-4:]})")

    def get_supported_countries(self) -> Dict[str, Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å—Ç—Ä–∞–Ω, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —ç—Ç–∏–º –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–æ–º."""
        return {
            code: self.adzuna_countries.get(code, {"name": name})
            for code, name in self.country_map.items()
        }

    def search_jobs(self, preferences: Dict) -> List[JobVacancy]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞. –ò—â–µ—Ç –ø–æ —Å—Ç—Ä–∞–Ω–µ, –∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –≥–æ—Ä–æ–¥ - —É—Ç–æ—á–Ω—è–µ—Ç –ø–æ–∏—Å–∫.
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –¥–ª—è –∫–∞–∂–¥–æ–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –æ—Ç–¥–µ–ª—å–Ω–æ.
        """
        logger.info(f"üîç {self.source_name}: –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫...")
        all_jobs: List[JobVacancy] = []
        
        selected_jobs = preferences.get('selected_jobs', [])
        countries = preferences.get('countries', [])
        cities = preferences.get('cities', [])

        if not selected_jobs or not countries:
            return []

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        total_searches = len(selected_jobs) * len(countries) * (len(cities) if cities else 1)
        current_search = 0

        for russian_job_title in selected_jobs:
            # –ü–æ–ª—É—á–∞–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏
            english_keywords = self._get_english_keywords(russian_job_title)
            if not english_keywords:
                continue

            keywords = " ".join(english_keywords)
            logger.info(f"‚ÑπÔ∏è {self.source_name}: –ò—â–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏—é '{russian_job_title}' –ø–æ —Å–ª–æ–≤–∞–º: '{keywords}'")

            for country_code in countries:
                country_name_for_api = self.country_map.get(country_code)
                if not country_name_for_api:
                    logger.warning(f"–°—Ç—Ä–∞–Ω–∞ {country_code} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
                    continue

                if cities:
                    for city in cities:
                        current_search += 1
                        search_location = f"{city}, {country_name_for_api}"
                        logger.info(f"üìç –ü–æ–∏—Å–∫ {current_search}/{total_searches}: {search_location}")
                        jobs = self._fetch_all_pages(keywords, search_location, country_code)
                        all_jobs.extend(jobs)
                else:
                    current_search += 1
                    search_location = country_name_for_api
                    logger.info(f"üìç –ü–æ–∏—Å–∫ {current_search}/{total_searches}: {search_location}")
                    jobs = self._fetch_all_pages(keywords, search_location, country_code)
                    all_jobs.extend(jobs)

        unique_jobs = self._deduplicate_jobs(all_jobs)
        logger.info(f"‚úÖ {self.source_name}: –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π: {len(unique_jobs)} –∏–∑ {len(all_jobs)} –æ–±—â–∏—Ö.")
        return unique_jobs

    def _get_english_keywords(self, russian_job_title: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏.
        """
        english_keywords = []
        
        for category in self.specific_jobs_map.values():
            if russian_job_title in category:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 —Ç–µ—Ä–º–∏–Ω–∞ (–æ–Ω–∏ –≤—Å–µ–≥–¥–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º)
                english_terms = category[russian_job_title][:3]
                english_keywords.extend([term for term in english_terms if term])
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∞–º–æ –Ω–∞–∑–≤–∞–Ω–∏–µ
        if not english_keywords:
            english_keywords.append(russian_job_title)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        seen = set()
        unique_keywords = []
        for keyword in english_keywords:
            if keyword not in seen:
                seen.add(keyword)
                unique_keywords.append(keyword)
        
        return unique_keywords

    def _fetch_all_pages(self, keywords: str, location: str, country_code: str) -> List[JobVacancy]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ —Å–æ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.
        """
        page = 1
        total_jobs: List[JobVacancy] = []
        consecutive_empty_pages = 0  # –°—á–µ—Ç—á–∏–∫ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–¥—Ä—è–¥
        
        while page <= self.MAX_PAGES_PER_SEARCH:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
            search_params = self._build_search_params(keywords, location, page)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
            cache_key_params = {k: v for k, v in search_params.items() if k not in ['user_ip', 'user_agent']}
            cached_result = self.cache_manager.get_cached_result(cache_key_params)
            
            if cached_result:
                total_jobs.extend(cached_result)
                if len(cached_result) < self.JOBS_PER_PAGE:
                    break  # –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                page += 1
                continue

            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —Å rate limiting
            self.rate_limiter.wait_if_needed()
            
            jobs_on_page = self._fetch_single_page(search_params, country_code, keywords)
            
            if jobs_on_page is None:
                # –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ
                break
            
            if not jobs_on_page:
                # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                consecutive_empty_pages += 1
                if consecutive_empty_pages >= 2:
                    break  # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º, –µ—Å–ª–∏ 2 –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–¥—Ä—è–¥
            else:
                consecutive_empty_pages = 0
                total_jobs.extend(jobs_on_page)
                self.cache_manager.cache_result(cache_key_params, jobs_on_page)
                logger.info(f"üìÑ {self.source_name}: –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} - –Ω–∞–π–¥–µ–Ω–æ {len(jobs_on_page)} –≤–∞–∫–∞–Ω—Å–∏–π")
                
                if len(jobs_on_page) < self.JOBS_PER_PAGE:
                    break  # –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
            
            page += 1
            if page <= self.MAX_PAGES_PER_SEARCH:
                time.sleep(self.DELAY_BETWEEN_PAGES)
                
        return total_jobs

    def _build_search_params(self, keywords: str, location: str, page: int) -> Dict:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è API –∑–∞–ø—Ä–æ—Å–∞."""
        return {
            'affid': self.affid,
            'keywords': keywords,
            'location': location,
            'page': page,
            'sort': 'date',
            'user_ip': '127.0.0.1',
            'user_agent': self.user_agent
        }

    def _fetch_single_page(self, search_params: Dict, country_code: str, keywords: str) -> Optional[List[JobVacancy]]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç None –ø—Ä–∏ –æ—à–∏–±–∫–µ, –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –µ—Å–ª–∏ –Ω–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–π.
        """
        try:
            response = self.session.get(
                self.base_url, 
                params=search_params, 
                timeout=self.REQUEST_TIMEOUT
            )
            
            if response.status_code != 200:
                logger.error(f"{self.source_name} API –æ—à–∏–±–∫–∞ {response.status_code}: {response.text[:200]}")
                return None

            data = response.json()
            
            if data.get('type') == 'ERROR':
                logger.error(f"{self.source_name} API –≤–µ—Ä–Ω—É–ª–æ –æ—à–∏–±–∫—É: {data.get('error')}")
                return None

            jobs_on_page_raw = data.get('jobs', [])
            if not jobs_on_page_raw:
                return []
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏
            country_name = self._get_country_name_by_code(country_code)
            normalized_jobs = []
            
            for job_data in jobs_on_page_raw:
                job = self._normalize_job_data(job_data, country_name, keywords)
                if job:
                    normalized_jobs.append(job)
            
            return normalized_jobs

        except requests.Timeout:
            logger.warning(f"{self.source_name}: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è {search_params.get('location')}")
            return None
        except requests.RequestException as e:
            logger.error(f"{self.source_name}: –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {e}")
            return None
        except Exception as e:
            logger.error(f"{self.source_name}: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {e}")
            return None

    def _normalize_job_data(self, raw_job: Dict, country_name: str, search_term: str) -> Optional[JobVacancy]:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç API –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç JobVacancy.
        """
        try:
            title = raw_job.get('title', '').strip()
            description = raw_job.get('description', '').strip()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            if not self.is_relevant_job(title, description, search_term):
                return None

            url = raw_job.get('url')
            if not url:
                return None
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
            job_id = hashlib.md5(url.encode()).hexdigest()

            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
            posted_date = self._parse_date(raw_job.get('date', ''))

            return JobVacancy(
                id=f"careerjet_{job_id}",
                title=title,
                company=(raw_job.get('company') or 'Not specified').strip(),
                location=(raw_job.get('locations') or 'Not specified').strip(),
                salary=raw_job.get('salary'),
                description=description,
                apply_url=url,
                source=self.source_name,
                posted_date=posted_date,
                country=country_name,
                job_type=None,
                language_requirement=self.determine_language_requirement(title, description),
                refugee_friendly=self.is_refugee_friendly(title, description, search_term)
            )
        except Exception as e:
            logger.warning(f"{self.source_name}: –û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏: {e}")
            return None

    def _parse_date(self, date_str: str) -> str:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤."""
        if not date_str:
            return datetime.now().strftime('%Y-%m-%d')
        
        try:
            # –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç Careerjet
            return datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S %Z').strftime('%Y-%m-%d')
        except (ValueError, TypeError):
            # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
            for fmt in ['%Y-%m-%d', '%d/%m/%Y', '%m/%d/%Y']:
                try:
                    return datetime.strptime(date_str[:10], fmt).strftime('%Y-%m-%d')
                except:
                    continue
            return datetime.now().strftime('%Y-%m-%d')

    def is_relevant_job(self, job_title: str, job_description: str, search_term: str) -> bool:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å —É—á–µ—Ç–æ–º –æ–ø–∏—Å–∞–Ω–∏—è.
        """
        search_terms = set(term.lower() for term in search_term.split() if len(term) > 2)
        title_lower = job_title.lower()
        description_lower = job_description.lower()[:500]  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ –æ–ø–∏—Å–∞–Ω–∏—è
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
        title_match = any(term in title_lower for term in search_terms)
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ, –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ (–Ω–æ —Ç—Ä–µ–±—É–µ–º –±–æ–ª—å—à–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π)
        if not title_match and len(search_terms) > 1:
            description_matches = sum(1 for term in search_terms if term in description_lower)
            return description_matches >= len(search_terms) // 2
        
        return title_match

    def _get_country_name_by_code(self, country_code: str) -> str:
        """
        –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –∏–º–µ–Ω–∏ —Å—Ç—Ä–∞–Ω—ã –ø–æ –∫–æ–¥—É.
        """
        return self.adzuna_countries.get(country_code, {}).get('name', country_code.upper())

    def _deduplicate_jobs(self, jobs: List[JobVacancy]) -> List[JobVacancy]:
        """
        –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–º—É –∫–ª—é—á—É —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥.
        """
        seen: Set[str] = set()
        unique_jobs: List[JobVacancy] = []
        
        for job in jobs:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º URL –∫–∞–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á
            if job.apply_url not in seen:
                seen.add(job.apply_url)
                unique_jobs.append(job)
        
        return unique_jobs


if __name__ == '__main__':
    print("üöÄ –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ CareerjetAggregator...")
    
    try:
        main_aggregator_for_test = GlobalJobAggregator()
        aggregator = CareerjetAggregator(
            adzuna_countries=main_aggregator_for_test.countries,
            specific_jobs_map=main_aggregator_for_test.specific_jobs
        )
    except Exception as e:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∞: {e}")
        aggregator = None

    if aggregator:
        test_preferences = {
            'selected_jobs': ['–ó–∞–ø—Ä–∞–≤—â–∏–∫ –Ω–∞ –ê–ó–°', '–û–ø–µ—Ä–∞—Ç–æ—Ä –ê–ó–°'],
            'countries': ['us'],
            'cities': []  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –±–µ–∑ –≥–æ—Ä–æ–¥–∞
        }
        
        start_time = time.time()
        found_jobs = aggregator.search_jobs(test_preferences)
        end_time = time.time()
        
        print(f"\n\n--- –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ê ---")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(found_jobs)}")
        
        if found_jobs:
            print("\nüìù –ü—Ä–∏–º–µ—Ä 5 –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π:")
            for i, job in enumerate(found_jobs[:5], 1):
                print(f"  {i}. {job.title} –≤ {job.company} ({job.location})")
                print(f"     –°—Å—ã–ª–∫–∞: {job.apply_url}")
                print(f"     –î–∞—Ç–∞: {job.posted_date}")