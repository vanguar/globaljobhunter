#!/usr/bin/env python3
"""
Remotive Aggregator for GlobalJobHunter v1.3
"""

import os
import requests
import time
from datetime import datetime
from typing import List, Dict, Optional
import hashlib

# --- –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑ adzuna_aggregator ---
from adzuna_aggregator import JobVacancy, CacheManager, RateLimiter

# --- –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã ---
from base_aggregator import BaseJobAggregator

class RemotiveAggregator(BaseJobAggregator):
    """
    –ê–≥—Ä–µ–≥–∞—Ç–æ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π —á–µ—Ä–µ–∑ Remotive API.
    - –£–õ–£–ß–®–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω —á–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –º–æ–≥—É—Ç –±—ã—Ç—å —É–¥–∞–ª–µ–Ω–Ω—ã–º–∏.
    - –ò–°–ü–†–ê–í–õ–ï–ù–û: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è Rate Limit.
    - –£–ª—É—á—à–µ–Ω–æ: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω Rate Limiter.
    """
    
    # --- –ù–û–í–û–ï: –°–ø–∏—Å–æ–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –∏—â–µ–º –Ω–∞ —ç—Ç–æ–º —Å–∞–π—Ç–µ ---
    NON_REMOTE_JOBS = {
        # –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∏ –¥–æ—Å—Ç–∞–≤–∫–∞
        '–í–æ–¥–∏—Ç–µ–ª—å —Ç–∞–∫—Å–∏', '–í–æ–¥–∏—Ç–µ–ª—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ B', '–í–æ–¥–∏—Ç–µ–ª—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ C',
        '–í–æ–¥–∏—Ç–µ–ª—å-–∫—É—Ä—å–µ—Ä', '–ö—É—Ä—å–µ—Ä –ø–µ—à–∫–æ–º', '–ö—É—Ä—å–µ—Ä-–¥–æ—Å—Ç–∞–≤—â–∏–∫ –µ–¥—ã',
        '–í–æ–¥–∏—Ç–µ–ª—å –∞–≤—Ç–æ–±—É—Å–∞', '–í–æ–¥–∏—Ç–µ–ª—å –≥—Ä—É–∑–æ–≤–∏–∫–∞',
        # –ê–≤—Ç–æ—Å–µ—Ä–≤–∏—Å
        '–ê–≤—Ç–æ–º–µ—Ö–∞–Ω–∏–∫', '–ê–≤—Ç–æ—Å–ª–µ—Å–∞—Ä—å', '–®–∏–Ω–æ–º–æ–Ω—Ç–∞–∂–Ω–∏–∫', '–î–∏–∞–≥–Ω–æ—Å—Ç',
        '–ú–∞—Å—Ç–µ—Ä-–ø—Ä–∏—ë–º—â–∏–∫', '–ö—É–∑–æ–≤—â–∏–∫', '–ú–∞–ª—è—Ä –ø–æ –∞–≤—Ç–æ',
        # –ê–ó–° –∏ –¢–æ–ø–ª–∏–≤–æ
        '–ó–∞–ø—Ä–∞–≤—â–∏–∫ –Ω–∞ –ê–ó–°', '–û–ø–µ—Ä–∞—Ç–æ—Ä –ê–ó–°', '–ö–∞—Å—Å–∏—Ä –Ω–∞ –ê–ó–°',
        # –ù–µ—Ñ—Ç—å –∏ –≥–∞–∑
        '–û–ø–µ—Ä–∞—Ç–æ—Ä –¥–æ–±—ã—á–∏', '–ü–æ–º–æ—â–Ω–∏–∫ –±—É—Ä–∏–ª—å—â–∏–∫–∞', '–†–∞–±–æ—á–∏–π –Ω–µ—Ñ—Ç–µ–±–∞–∑—ã',
        # –°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ
        '–°—Ç—Ä–æ–∏—Ç–µ–ª—å-—Ä–∞–∑–Ω–æ—Ä–∞–±–æ—á–∏–π', '–ì—Ä—É–∑—á–∏–∫', '–°–∫–ª–∞–¥—Å–∫–æ–π —Ä–∞–±–æ—Ç–Ω–∏–∫',
        '–†–∞–∑–Ω–æ—Ä–∞–±–æ—á–∏–π', '–†–∞–±–æ—á–∏–π –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ',
        # –û–±—â–µ–ø–∏—Ç –∏ —Å–µ—Ä–≤–∏—Å
        '–û—Ñ–∏—Ü–∏–∞–Ω—Ç', '–ë–∞—Ä–º–µ–Ω', '–ü–æ–≤–∞—Ä', '–ü–æ–º–æ—â–Ω–∏–∫ –ø–æ–≤–∞—Ä–∞', '–ü–æ—Å—É–¥–æ–º–æ–π—â–∏–∫',
        '–ö–∞—Å—Å–∏—Ä', '–ü—Ä–æ–¥–∞–≤–µ—Ü',
        # –°–µ—Ä–≤–∏—Å –∏ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ
        '–£–±–æ—Ä—â–∏–∫', '–°–∞–¥–æ–≤–Ω–∏–∫', '–î–æ–º—Ä–∞–±–æ—Ç–Ω–∏—Ü–∞', '–ú–∞—Å—Å–∞–∂–∏—Å—Ç',
        # –£—Ö–æ–¥ –∏ –º–µ–¥–∏—Ü–∏–Ω–∞ (—Ç—Ä–µ–±—É—é—â–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏—è)
        '–ú–µ–¥—Å–µ—Å—Ç—Ä–∞', '–°–∏–¥–µ–ª–∫–∞', '–ù—è–Ω—è', '–ì—É–≤–µ—Ä–Ω–∞–Ω—Ç–∫–∞', '–£—Ö–æ–¥ –∑–∞ –ø–µ–Ω—Å–∏–æ–Ω–µ—Ä–∞–º–∏'
    }

    def __init__(self, specific_jobs_map: Dict, cache_duration_hours: int = 12):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞.
        """
        super().__init__(source_name='Remotive')
        self.base_url = "https://remotive.com/api/remote-jobs"
        self.specific_jobs_map = specific_jobs_map
        self.cache_manager = CacheManager(cache_duration_hours=cache_duration_hours)
        self.rate_limiter = RateLimiter(requests_per_minute=2) 

        self.job_to_category_map = {
            'python developer': 'software-dev', 'web developer': 'software-dev',
            'programmer': 'software-dev', 'software developer': 'software-dev',
            'qa engineer': 'qa', 'software tester': 'qa', 'data analyst': 'data',
            'data scientist': 'data', 'designer': 'design', 'product manager': 'product',
            'manager': 'management', 'sales assistant': 'sales-marketing',
            'marketer': 'sales-marketing', 'recruiter': 'hr',
            'customer support': 'customer-service'
        }
        print(f"‚úÖ Remotive Aggregator v1.3 –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (Rate Limit: 2/min, —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–π).")

    def get_supported_countries(self) -> Dict[str, Dict]:
        return {}

    def search_jobs(self, preferences: Dict, progress_callback=None, cancel_check=None) -> List[JobVacancy]:
        """
        Remotive: –±—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º.
        - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ progress_callback(list[JobVacancy]) ‚Äî –æ—Ç–¥–∞—ë–º –±–∞—Ç—á–∏.
        - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ cancel_check() ‚Äî –º—è–≥–∫–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞.
        - –û–±—â–∏–π —Ç–∞–π–º-–±—é–¥–∂–µ—Ç –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 20—Å, –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å REMOTIVE_MAX_RUNTIME).
        """
        print(f"üì° {self.source_name}: –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π...")
        all_jobs: List[JobVacancy] = []

        selected_jobs = preferences.get('selected_jobs', [])
        if not selected_jobs:
            return []

        # –æ–±—â–∏–π –±—é–¥–∂–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ Remotive
        MAX_RUNTIME_SEC = int(os.getenv("REMOTIVE_MAX_RUNTIME", "20"))
        started_at = time.time()

        for russian_job_title in selected_jobs:
            if cancel_check and cancel_check():
                return self._deduplicate_jobs(all_jobs)
            if time.time() - started_at > MAX_RUNTIME_SEC:
                print(f"‚è≥ {self.source_name}: –ø—Ä–µ–≤—ã—à–µ–Ω —Ç–∞–π–º-–±—é–¥–∂–µ—Ç {MAX_RUNTIME_SEC}s ‚Äî –ø—Ä–µ—Ä—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∑–∞–ø—Ä–æ—Å—ã")
                break

            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —è–≤–Ω—ã–µ ¬´–Ω–µ-—É–¥–∞–ª—ë–Ω–Ω—ã–µ¬ª –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏
            if russian_job_title in self.NON_REMOTE_JOBS:
                print(f"    - –ü—Ä–æ–ø—É—Å–∫–∞–µ–º '{russian_job_title}' (–Ω–µ remote)")
                continue

            english_keywords = self._get_english_keywords(russian_job_title)
            if not english_keywords:
                continue

            primary_keyword = english_keywords[0]
            category = self.job_to_category_map.get(primary_keyword.lower())

            if category:
                print(f"    - –ò—â–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}' –¥–ª—è '{russian_job_title}'")
                jobs = self._fetch_jobs(
                    params={'category': category},
                    progress_callback=progress_callback,
                    cancel_check=cancel_check,
                    started_at=started_at,
                    max_runtime_sec=MAX_RUNTIME_SEC
                )
            else:
                search_query = " ".join(english_keywords)
                print(f"    - –ò—â–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: '{search_query}'")
                jobs = self._fetch_jobs(
                    params={'search': search_query},
                    progress_callback=progress_callback,
                    cancel_check=cancel_check,
                    started_at=started_at,
                    max_runtime_sec=MAX_RUNTIME_SEC
                )

            if jobs:
                all_jobs.extend(jobs)

            if cancel_check and cancel_check():
                return self._deduplicate_jobs(all_jobs)

        print(f"‚úÖ {self.source_name}: –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à—ë–Ω. –ù–∞–π–¥–µ–Ω–æ –≤—Å–µ–≥–æ: {len(all_jobs)}.")
        return self._deduplicate_jobs(all_jobs)




    def _get_english_keywords(self, russian_job_title: str) -> List[str]:
        for category in self.specific_jobs_map.values():
            if russian_job_title in category:
                return [term for term in category[russian_job_title][:3] if term]
        return []

    def _fetch_jobs(
    self,
    params: Dict,
    progress_callback=None,
    cancel_check=None,
    started_at: float = None,
    max_runtime_sec: int = 20
) -> List[JobVacancy]:
        """
        –û–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∫ API Remotive (–±—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º):
        - Cache ‚Üí –æ—Ç–¥–∞—Ç—å –±–∞—Ç—á.
        - –ö–æ—Ä–æ—Ç–∫–∏–π HTTP-—Ç–∞–π–º–∞—É—Ç.
        - 429/timeout: –º–∏–∫—Ä–æ–ø–∞—É–∑–∞ 150‚Äì400–º—Å –∏ —Å—Ä–∞–∑—É –¥–∞–ª—å—à–µ (–±–µ–∑ –¥–æ–ª–≥–∏—Ö –æ–∂–∏–¥–∞–Ω–∏–π –∏ —Ä–µ—Ç—Ä–∞–µ–≤).
        """
        if cancel_check and cancel_check():
            return []
        if started_at and (time.time() - started_at > max_runtime_sec):
            return []

        # 0) –∫–µ—à
        cached_result = self.cache_manager.get_cached_result(params)
        if cached_result:
            tag = params.get('search') or params.get('category')
            print(f"    - Cache HIT –¥–ª—è '{tag}'. –ù–∞–π–¥–µ–Ω–æ: {len(cached_result)}.")
            if progress_callback and cached_result:
                try:
                    progress_callback(cached_result)
                except Exception:
                    pass
            return cached_result

        # 1) –∫–æ–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–π "–ª–∏–º–∏—Ç–µ—Ä" (—É —Ç–µ–±—è –æ–Ω —É–∂–µ –±—ã—Å—Ç—Ä—ã–π)
        ok = self.rate_limiter.wait_if_needed(cancel_check=cancel_check)
        if ok is False or (cancel_check and cancel_check()):
            return []
        if started_at and (time.time() - started_at > max_runtime_sec):
            return []

        HTTP_TIMEOUT = int(os.getenv("REMOTIVE_HTTP_TIMEOUT", "6"))

        try:
            response = requests.get(self.base_url, params=params, timeout=HTTP_TIMEOUT)
            if response.status_code == 200:
                data = response.json()
                jobs_raw = data.get('jobs', [])
                tag = params.get('search') or params.get('category')

                normalized_jobs: List[JobVacancy] = []
                for job_data in jobs_raw:
                    if cancel_check and cancel_check():
                        break
                    job = self._normalize_job_data(job_data, tag)
                    if job:
                        normalized_jobs.append(job)

                self.cache_manager.cache_result(params, normalized_jobs)
                print(f"    - –ù–∞–π–¥–µ–Ω–æ –∏ –∑–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–æ: {len(normalized_jobs)} –¥–ª—è '{tag}'.")

                if progress_callback and normalized_jobs:
                    try:
                        progress_callback(normalized_jobs)
                    except Exception:
                        pass

                return normalized_jobs

            if response.status_code == 429:
                tag = params.get('search') or params.get('category')
                backoff_ms = 200 + random.randint(0, 200)  # 200‚Äì400–º—Å
                print(f"‚õî Remotive: 429 Too Many Requests ‚Äî –º–∏–∫—Ä–æ–ø–∞—É–∑–∞ {backoff_ms} –º—Å –¥–ª—è '{tag}' –∏ –¥–≤–∏–≥–∞–µ–º—Å—è –¥–∞–ª—å—à–µ")
                yield_briefly(base_ms=backoff_ms, jitter_ms=0, cancel_check=cancel_check)
                return []

            print(f"‚ùå {self.source_name} API –æ—à–∏–±–∫–∞ {response.status_code}: {response.text[:200]}")
            return []

        except requests.Timeout:
            tag = params.get('search') or params.get('category')
            backoff_ms = 150 + random.randint(0, 200)  # 150‚Äì350–º—Å
            print(f"‚ö†Ô∏è {self.source_name}: —Ç–∞–π–º–∞—É—Ç –¥–ª—è '{tag}' ‚Äî –º–∏–∫—Ä–æ–ø–∞—É–∑–∞ {backoff_ms} –º—Å –∏ –¥–∞–ª—å—à–µ")
            yield_briefly(base_ms=backoff_ms, jitter_ms=0, cancel_check=cancel_check)
            return []
        except Exception as e:
            print(f"‚ùå {self.source_name}: –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            return []



    def _normalize_job_data(self, raw_job: Dict, search_term: str) -> Optional[JobVacancy]:
        try:
            title = raw_job.get('title', '')
            description = raw_job.get('description', '')
            
            url = raw_job.get('url')
            if not url:
                return None
            job_id = hashlib.md5(url.encode()).hexdigest()

            date_str = raw_job.get('publication_date', '')
            try:
                posted_date = datetime.fromisoformat(date_str.replace('Z', '+00:00')).strftime('%Y-%m-%d')
            except (ValueError, TypeError):
                posted_date = datetime.now().strftime('%Y-%m-%d')

            return JobVacancy(
                id=f"remotive_{job_id}",
                title=title,
                company=raw_job.get('company_name', 'Not specified'),
                location=raw_job.get('candidate_required_location', 'Worldwide'),
                salary=raw_job.get('salary'),
                description=description,
                apply_url=url,
                source=self.source_name,
                posted_date=posted_date,
                country='Remote',
                job_type=raw_job.get('job_type'),
                language_requirement=self.determine_language_requirement(title, description),
                refugee_friendly=self.is_refugee_friendly(title, description, search_term)
            )
        except Exception as e:
            print(f"‚ö†Ô∏è {self.source_name}: –û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏: {e}")
            return None

    def is_relevant_job(self, job_title: str, job_description: str, search_term: str) -> bool:
        """–ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å."""
        if search_term in self.job_to_category_map.values():
             return True
        
        search_keywords = search_term.lower().split()
        title_lower = job_title.lower()
        return any(keyword in title_lower for keyword in search_keywords)

    def _deduplicate_jobs(self, jobs: List[JobVacancy]) -> List[JobVacancy]:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ URL –≤–∞–∫–∞–Ω—Å–∏–∏."""
        seen = set()
        unique_jobs = []
        for job in jobs:
            if job.apply_url not in seen:
                seen.add(job.apply_url)
                unique_jobs.append(job)
        return unique_jobs